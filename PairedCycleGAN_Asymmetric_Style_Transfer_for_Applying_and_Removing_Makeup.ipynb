{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1zh7iSUtTZ1s2qIhBtejtvTOmap321ZOq",
      "authorship_tag": "ABX9TyP+ZMu5CP77WrgtlHzVS5iA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamesMum/paper_summary/blob/master/PairedCycleGAN_Asymmetric_Style_Transfer_for_Applying_and_Removing_Makeup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CK7JTGzHfrl",
        "colab_type": "text"
      },
      "source": [
        "#[Link to Original Paper](https://gfx.cs.princeton.edu/pubs/Chang_2018_PAS/Chang-CVPR-2018.pdf)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1UsBvn-eW9wIrCAPmgOCDDp_UoQ-qkPhB)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtHXcEna1Xxi",
        "colab_type": "text"
      },
      "source": [
        "# The Author(s) Goal\n",
        "Transforming the style of one image to another from a reference photo. Here they chose to change the makeup of a person with different style of makeups from a reference image.\n",
        "## Challanges\n",
        "- Difficulty in obtaining paired images, with and without makeup for the same person to compare it back with the results as ground truth (this is why an unsuppervised learning approach was taken)\n",
        "\n",
        "- Previous works implemented style transfere approaches. the results were unrealistic because it effected the original image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC1JYIun-oLp",
        "colab_type": "text"
      },
      "source": [
        "# The Elements of The Approach\n",
        "- Training two adversarial networks. One to that transferes makeup style and the other removes it and compare it to the orginal source image. \n",
        "This way the image will remain as close as possible to the original image while transfered to a new style (with makeup)\n",
        "Solving the problem of ground truth triple dataset.\n",
        "\n",
        "- The following is a similar architecture that is used to transform low resolution images to a high one\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vkjYf-8dhWW3ffqygV8GI1WRgXgKAn1G\" alt=\"drawing\" width=\"500\"/></center>\n",
        "\n",
        "\n",
        "The contribution of this paper:\n",
        "1- A feed forward transformation network that can reference a makeup photo to a source image\n",
        "2-  An asymmetric makeup\n",
        "transfer framework, wherein a trained makeup removal network works joinly with the transfere network to presearve the identity of the source image, augmented by the style dicriminator.\n",
        "3- New high quality dataset for before and after makeup images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSZKM0qDE06W",
        "colab_type": "text"
      },
      "source": [
        "## Formulation\n",
        "\n",
        "- Let X na Y be two image domains, with and without make-up. Note that there is no pairing exists between the pair.\n",
        "\n",
        "- We have two collection of images to categorized each domain.\n",
        "\n",
        "-  X a collection of no makeup portraits $x_i \\in X$.\n",
        "\n",
        "- Y a collection of portraits with makeup $y_j \\in Y$, where the makeup style ranges from nude and natural to artistic and intense. $Y^\\beta \\subset Y$ where $\\beta$ denotes the makeup style \n",
        "\n",
        "\n",
        "- training two seperate neural network G and F, one to transfere specific makeup style and other to remove the makeup respectively.\n",
        "\n",
        "- Training G on diverse trauining examples, hopfully it will generalize it learning from the entire with makeup image domain and will be able to transfer arbitrary makeup tyles to arbitrary faces at run time.\n",
        "\n",
        "- $G(x, y^\\beta)$\n",
        "Where $x \\in X$ an photo of a person without makeup and $y^\\beta \\in Y^\\beta$ where $y^\\beta$ is an image of another person with makeup.\n",
        "Basically G extract the make up from $y^\\beta$ and apply it to x maintaining its identity.\n",
        "\n",
        "- The resulting $G(x, y^\\beta)$ should be belong to the domain $y^\\beta$\n",
        "\n",
        "- F on the other hand learns to remove the make up giving the same photo $y^\\beta$ this time maintaining the last identity.\n",
        "\n",
        "- Bothe G and F are unbalanced. G takes input and referrence while F takes a with makeup example. \n",
        "\n",
        "- If both are succeded the output of F(G(x)) will result in the original source image without the makeup.\n",
        "\n",
        "### Losses\n",
        "- **The adversarial loss for G: $L_G$**\n",
        "\n",
        "where the discriminator Dy tries to discriminate between the real samples from domain Y and the generated samples G(x, y), and G aims to generate images that is indistiguishable by the Dy.\n",
        "\n",
        "- **The adversarial loss for F: $L_F$**\n",
        "F is encouraged to generate images that is indistinguishable from the no makeup faces samples from domain X.\n",
        "\n",
        "\n",
        "- **Identity Loss: $L_I$**\n",
        "The adverarial loss help us get closer to map the image from domain $x$ to domain $y^\\beta$, but there is no way here to make sure that we are not losing the identity of the image which is $x$ here.\n",
        "\n",
        "Using L1 loss to penalize the difference between F(G(x,y)) and x.\n",
        "\n",
        "- **Style Loss:**\n",
        "The previous losses make sure that the G lie in the manifold of Y (faces with makeup) while maintaining the identity of X. \n",
        "But we need to make sure that the details of a particular style $y^\\beta$. for this there are two prposed losses L1 reconreconstruction loss $L_s$ and style discriminator loss $L_p$.\n",
        "the key here that if we tarnsfer the makeup style $y^\\beta$ to face $x$ and then use the result of $G(x, y^\\beta$ to transfer the same makeup style back to the makeup removed face $F(y^\\beta)$ then the results of\n",
        "$G(F(y^\\beta\n",
        "), G(x, y\\beta\n",
        "))$ should look exactly like the input face with makeup.\n",
        "where G(x, yβ)\n",
        "transfere makeup style from yβ to face x (makeup to no makeup style)\n",
        "and F(yβ) used the result  to transfer the same  makeup style back to the makeup removed face.\n",
        "This loss though is incapable ot transfering sharp edges leading to a blury results. \n",
        "\n",
        "-For this reason we added an auxiliary discriminator $D_s$ that decides whether a given pair of faces wear the same makeup.\n",
        "it needs to be fed during training two pairs of images. The Real makeup pair ($y^\\beta$, the same makeup style applied to another face) and fake makeup pair ($y^\\beta, G(x,y^\\beta))$ \n",
        "\n",
        "- The challanger here is that there is no ground truth for the real makeup paires. Therefore a synthetic ground truth is generated using $W(x, y^\\beta)$. \"by warping the reference makeup face $y^\\beta$\n",
        "to match the detected facial landmarks in the source face $x$.\n",
        "\n",
        "**Total Loss**\n",
        "$L = \\lambda_GLG + \\lambda_F LF + LI + LS + \\lambda_P LP$\n",
        "\n",
        "\"$\\lambda_G$,$\\lambda_F$ , and $\\lambda_P$ are the weights to balance the multiple\n",
        "objectives.\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YR5cV0aqbSP",
        "colab_type": "text"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "- Three generators were trained seperately by focusing the network capacity on the unique characteristics of each individual regions, the eye, lip and skin.\n",
        "One generator for both of the eys was trained and the right eye was flipped horizontally.\n",
        "\n",
        "- Applying face parsing algorithm to segment out each of the facial component, e.g. eyes, eyebrows, nose, etc.\n",
        "\n",
        "For data collection check the original paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Bc4hgD-vbi",
        "colab_type": "text"
      },
      "source": [
        "# How and Where it can be Useful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7KKDwK1-z6S",
        "colab_type": "text"
      },
      "source": [
        "# Other Refrences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1zus8VA-tH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}